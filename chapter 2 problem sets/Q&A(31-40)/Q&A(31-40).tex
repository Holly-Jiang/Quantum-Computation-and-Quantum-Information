\documentclass[UTF8]{ctexart}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{geometry}
 \usepackage{indentfirst} 
 %\mathinner{\langle a | }\quad \mathinner{ | b \rangle}
% \def\bra#1{\mathinner{\langle{#1}}


 \setlength{\parindent}{2em}
\geometry{a4paper,scale=0.8}
\begin{document}
	\title{\textbf{Q\&A（2.31-2.40）}\\[1ex]\begin{large}
		\end{large}}
	\author{骆挺宇\quad 蒋慧}
	\maketitle
\begin{quote}
\textbf{Exercise 2.31: } Show that the tensor product of two positive operators is positive.
\\
\textbf{Answer:}\\
Suppose $\ket{v}, \ket{w}$ are arbitrary vectors of  the $V, W$ space, and $A, B$ are  two positive operators of  the $V, W$ space, respectively. Suppose  $\ket{v} \otimes \ket{w}$ is an arbitrary vectors of the $V \otimes W$ space.
\\
Then we can get the following derivation: \\
$(\ket{v} \otimes \ket{w}, A \otimes B (\ket{v} \otimes \ket{w}))$
$=(\ket{v} \otimes \ket{w}, A\ket{v} \otimes B\ket{w})$ 
$=(\ket{v} , A\ket{v})( \ket{w}, B\ket{w}) \geq 0.$
\\
Thus we proved that the tensor product of two positive operators is positive.
\\

\textbf{Exercise 2.32:  } Show that the tensor product of two projectors is a projector.
	\\ 
\textbf{Answer:}\\
	 Suppose $A, B$ are  two projectors of  the $V, W$ space, respectively, so it can be written as $A= \sum_{i} \ket{i} \bra{i}, B= \sum_{j} \ket{j} \bra{j}. $
	Thus we can get the following derivation: \\
	$(A\otimes B)^{2}=(A\otimes B)(A\otimes B)$\\
	 \hspace*{1.58cm}$=(AA\otimes BB)$\\
	 \hspace*{1.58cm}$=(A^{2}\otimes B^{2})$ \\
	 \hspace*{1.58cm}$=A \otimes B$.
	\\
	Thus we proved that the tensor product of two projectors is a projector.
	\\  \\
\textbf{Exercise 2.33:} The Hadamard operator on one qubit may be written as 
\begin{center}
$H=\frac{1}{\sqrt{2}}[(|0\rangle+|1\rangle)\langle 0|+(|0\rangle-|1\rangle)\langle 1|]$\\
\end{center}
Show explicitly that the Hadamard transform on $n$ qubits, $H\otimes n$, may be written as
 \begin{center}
$ H^{\otimes n}=\frac{1}{\sqrt{2^{n}}} \sum_{x, y}(-1)^{x \cdot y}|x\rangle\langle y|.$
 \end{center}
 Write out an explicit matrix representation for $H^{\otimes 2}$.\\
 
\textbf{Answer:}\\
	
	
	
\textbf{Exercise 2.34: } Find the square root and logarithm of the matrix	
$\begin{bmatrix}4& 3\\ 3&4\end{bmatrix}$.\\
\textbf{Answer:}\\
	 Since $A^{\dagger}=A, A $ is a Hermitian and normal.
	 $ A$ can be written as $A = 􏰞 \sum_{i}\lambda_{i} \ket{i}\bra{i}$. We first solve the eigenvalues and eigenvectors of A. 
	 We set 
	det$|A-\lambda E|=0,$ then $|A-\lambda E|=0,$ \\
	\\
			$\begin{bmatrix}4-\lambda & 3\\ 3&4-\lambda \end{bmatrix}=0.$\\
			\\
 The solution of $\lambda$ is 1 or 7.\\ 
 \\
 		 When $\lambda=1$,	calculate 		
		 $\begin{bmatrix}3 & 3\\ 3&3 \end{bmatrix}\begin{bmatrix}x_{1}\\ x_{2}\end{bmatrix}=0$, then we can get $3x_{1}+3x_{2}=0.$\\ 
		Suppose $x_{1}=1$, then  $x_{2}=-1.$ $\ket{\lambda_{1}}$ can be written as 
		$\ket{\lambda_{1}}=\begin{bmatrix}1\\ -1 \end{bmatrix}$.\\
		\\
	After normalization, the eigenvector is obtained:
		$\ket{\lambda_{1}}=\frac{1}{\sqrt{2}}\begin{bmatrix}1\\ -1 \end{bmatrix}.$
		\\  \\
		 		 When $\lambda=7$,	 calculate		
		 $\begin{bmatrix}-3 & 3\\ 3&-3 \end{bmatrix}\begin{bmatrix}x_{1}\\ x_{2}\end{bmatrix}=0$, then we get $3x_{1}-3x_{2}=0.$\\  \\
		Suppose $x_{1}=1$, then  $x_{2}=1, $ we can get  
		$\ket{\lambda_{7}}=\begin{bmatrix}1\\ 1 \end{bmatrix}.$\\
		\\
	After normalization, the eigenvector is obtained:
		$\ket{\lambda_{7}}=\frac{1}{\sqrt{2}}\begin{bmatrix}1\\ 1 \end{bmatrix}$.
		\\
			So the diagonal representations of $A$ is 
	$\frac{1}{2}\begin{bmatrix}1\\ -1 \end{bmatrix}\begin{bmatrix}1& -1 \end{bmatrix}+
	\frac{7}{2}\begin{bmatrix}1\\ 1 \end{bmatrix}\begin{bmatrix}1& 1 \end{bmatrix}.$
	\\ \\
	square root:\\ 
	\\
	$\sqrt{A}=\frac{1}{2}\begin{bmatrix}1\\ -1 \end{bmatrix}\begin{bmatrix}1& -1 \end{bmatrix}+
	\frac{\sqrt{7}}{2}\begin{bmatrix}1\\ 1 \end{bmatrix}\begin{bmatrix}1& 1 \end{bmatrix}
		=\frac{1}{2}\begin{bmatrix}\sqrt{7}+1 & \sqrt{7}-1\\ \sqrt{7}-1 & \sqrt{7}+1 \end{bmatrix}.$
		\\  \\
	logarithm:\\ 
	\\
		$\log_2A=\frac{1}{2}\log_27\begin{bmatrix}1 & 1\\ 1 &1\end{bmatrix}.$
		 \\ \\ \\
\textbf{Exercise 2.35: (Exponential of the Pauli matrices)} Let $ \vec{v}$ be any real, three-dimensional unit vector and $\theta$ a real number. Prove that 
\begin{center}
$\exp (i \theta \vec{v} \cdot \vec{\sigma})=\cos (\theta) I+i \sin (\theta) \vec{v} \cdot \vec{\sigma}$,
\end{center}
where $\vec{v} \cdot \vec{\sigma}=\sum_{i=1}^{3}\vec{v}_{i} \vec{\sigma}_{i}. $ This exercise is generalized in Problem 2.1 on page 117. 
\\
	\textbf{Answer:}\\
	According to the meaning of the question, we can get
 $\vec{v} \cdot \vec{\sigma}=\sum_{i=1}^{3}\vec{v}_{i} \vec{\sigma}_{i}$ \\ 
 \\
 $=v_{1}\begin{bmatrix}0& 1\\1& 0\end{bmatrix}+	
	v_{2}\begin{bmatrix}0& -i\\i&0\end{bmatrix}+
	v_{3}\begin{bmatrix}1& 0\\0&-1\end{bmatrix}
	=\begin{bmatrix}v_{3}& v_{1}-iv_{2}\\ v_{1}+iv_{2} &-v_{3}\end{bmatrix}$.
	\\ \\
	Since  $ \vec{v}$ be any real, three-dimensional unit vector, we can get $v_{1}^2+v_{2}^2+v_{3}^2=1$.\\
	We first solve the eigenvalues and eigenvectors of A. \\
	\\
	Let $\begin{bmatrix}v_{3}-\lambda& v_{1}-iv_{2}\\ v_{1}+iv_{2} &-v_{3}-\lambda \end{bmatrix}=0$, 
	we can get $\lambda^{2}=v_{1}^2+v_{2}^2+v_{3}^2$.\\
	\\
	Thus the solution of  $\lambda$ is $1$ or  $-1$. \\
	When $\lambda=1$, we set the eigenvector  is $\ket{\lambda_{1}}$, and when $\lambda=-1$, we set the eigenvector  is $\ket{\lambda_{-1}}$. Then we can get the following derivation: \\
	 $\vec{v} \cdot \vec{\sigma}=\ket{\lambda_{1}}\bra{\lambda_{1}}-\ket{\lambda_{-1}}\bra{\lambda_{-1}}$.
	 \\
	$(\vec{v} \cdot \vec{\sigma})^{\dagger}=\ket{\lambda_{1}}\bra{\lambda_{1}}-\ket{\lambda_{-1}}\bra{\lambda_{-1}}$.
	\\
	Since $\vec{v} \cdot \vec{\sigma}=(\vec{v} \cdot \vec{\sigma})^{\dagger}$, 
	 $\vec{v} \cdot \vec{\sigma}$ is Hermitian. \\
	$i\theta \vec{v} \cdot \vec{\sigma}
	=i\theta \ket{\lambda_{1}}\bra{\lambda_{1}}-i\theta \ket{\lambda_{-1}}\bra{\lambda_{-1}}$,
	\\
	exp$(i\theta \vec{v} \cdot \vec{\sigma})=e^{i\theta} \ket{\lambda_{1}}\bra{\lambda_{1}}+e^{-i\theta} \ket{\lambda_{-1}}\bra{\lambda_{-1}}$\\
	$=(\cos (\theta) +i\sin(\theta))\ket{\lambda_{1}}\bra{\lambda_{1}}+(\cos (\theta) -i\sin(\theta)) \ket{\lambda_{-1}}\bra{\lambda_{-1}}$
	\\
	$=\cos(\theta)(\ket{\lambda_{1}}\bra{\lambda_{1}}+\ket{\lambda_{-1}}\bra{\lambda_{-1}})+i\sin(\theta)(\ket{\lambda_{1}}\bra{\lambda_{1}}-\ket{\lambda_{-1}}\bra{\lambda_{-1}})$.
	\\
	According to the completeness relation, we can get $\ket{\lambda_{1}}\bra{\lambda_{1}}+\ket{\lambda_{-1}}\bra{\lambda_{-1}}=I$.
	\\
	Thus, we proved that $\exp (i \theta \vec{v} \cdot \vec{\sigma})=\cos (\theta) I+i \sin (\theta) \vec{v} \cdot \vec{\sigma}$.
	\\ \\
\textbf{Exercise 2.36:} Show that the Pauli matrices except for $I$ have trace zero. \\
\textbf{Answer:}\\
$\operatorname{tr}\left(\sigma_{0}\right)=\operatorname{tr}(\left[\begin{array}{ll}{1} & {0} \\ {0} & {1}\end{array}\right])=2$. \\
$\operatorname{tr}\left(\sigma_{1}\right)=\operatorname{tr}(\left[\begin{array}{ll}{0} & {1} \\ {1} & {0}\end{array}\right])=0.$\\
$\operatorname{tr}\left(\sigma_{2}\right)=\operatorname{tr}(\left[\begin{array}{ll}{0} & {-i} \\ {i} & {0}\end{array}\right])=0.$\\
$\operatorname{tr}\left(\sigma_{3}\right)=\operatorname{tr}(\left[\begin{array}{ll}{1} & {0} \\ {0} & {-1}\end{array}\right])=0.$\\
Above all, the Pauli matrices except for $I$ have trace zero.
\\ \\
\textbf{Exercise 2.37: (Cyclic property of the trace) }If A and B are two linear operators show that
		\begin{center}
		$\operatorname{tr}(AB)=\operatorname{tr}(BA)$. 
		\end{center}
	\textbf{Answer:}\\
		$\operatorname{tr}(AB)=\sum_{i}\bra{i}AB\ket{i}=\sum_{i}\bra{i}AIB\ket{i}=\sum_{ij}\bra{i}A\ket{j}\bra{j}B\ket{i}=\sum_{ij}\bra{j}B\ket{i}\bra{i}A\ket{j}$\\
		$=\sum_{j}\bra{j}BA\ket{j}=\operatorname{tr}(BA)$. \\
		Thus we proved that $\operatorname{tr}(AB)=\operatorname{tr}(BA)$. 
		\\ \\
\textbf{Exercise 2.38:  (Linearity of the trace) } If $A$ and $B$ are two linear operators, show that 
		\begin{center}
		$\operatorname{tr}(A+B)=\operatorname{tr}(A)+\operatorname{tr}(B)$ \\
		\end{center}
		and if $z$ is an arbitrary complex number show that
		\begin{center}
		$\operatorname{tr}(zA)=z\operatorname{tr}(A)$. \\
		\end{center}
\textbf{Answer:}\\
$\operatorname{tr}(A)=\sum_{i}(A)_{ii},
\operatorname{tr}(B)=\sum_{i}(B)_{ii}.$
\\
		$\operatorname{tr}(A+B)=\sum_{i}(A+B)_{ii}=\sum_{i}(A_{ii}+B_{ii})=\sum_{i}(A_{ii})+\sum_{i}(B_{ii})
		=\operatorname{tr}(A)+\operatorname{tr}(B).$ \\
		Thus we proved that 	$\operatorname{tr}(A+B)=\operatorname{tr}(A)+\operatorname{tr}(B)$. \\
		$\operatorname{tr}(zA)=\sum_{i}(zA)_{ii}=z\sum_{i}(A)_{ii}=z\operatorname{tr}(A).$ \\
		Thus we proved that $\operatorname{tr}(zA)=z\operatorname{tr}(A)$. \\
	\\ 
\textbf{Exercise 2.39: (The Hilbert–Schmidt inner product on operators)} The set $L_{v}$ of linear operators on a Hilbert space $V$ is obviously a vector space – the sum of two linear operators is a linear operator, $zA$ is a linear operator if $A$ is a linear operator and $z$ is a complex number, and there is a zero element $0$. An important additional result is that the vector space $L_{v}$ can be given a natural inner product structure, turning it into a Hilbert space.\\
(1) Show that the function (·, ·) on $L_{v}$× $L_{v}$ defined by 
\begin{center}
$(A, B)=\operatorname{tr}(A^{\dagger}B)$
\end{center}
is an inner product function. This inner product is known as the Hilbert–Schmidt or trace inner product. 
\\
(2) If $V$ has $d$ dimensions show that $L_{v}$ has dimension $d^{2}$. 
\\
(3) Find an orthonormal basis of Hermitian matrices for the Hilbert space $L_{v}$. 
\\
\textbf{Answer:}\\
(1) Prove as follows: \\
According to the definition of inner product:\\
1. $(·, ·)$ is linear in the second argument.\\
$(A,\sum_{i}\lambda_{i}B_{i})=\operatorname{tr}(A^{\dagger}\sum_{i}\lambda_{i}B_{i})=\operatorname{tr}(\sum_{i}\lambda_{i}A^{\dagger}B_{i})=\sum_{i}\lambda_{i}\operatorname{tr}(A^{\dagger}B_{i}).$\\
Thus $(·, ·)$ is linear in the second argument.
\\
2. $(\ket{v},\ket{w})=(\ket{w},\ket{v})^{*}$. \\
$(A,B)^{*}=(\operatorname{tr}(A^{\dagger}B))^{*}
=(\sum_{ij}\bra{i}A\ket{j}\bra{j}B\ket{i})^{*}
=\sum_{ij}(\bra{j}B\ket{i})^{*}(\bra{i}A^{\dagger}\ket{j})^{*}
=\sum_{ij}(\bra{i}B^{\dagger}\ket{j})(\bra{j}A\ket{i})\\
=\sum_{i}\bra{i}B^{\dagger}A\ket{i}
=\operatorname{tr}(B^{\dagger}A)
=(B,A).
$\\
Thus we proved that $(\ket{v},\ket{w})=(\ket{w},\ket{v})^{*}$. 
\\
3. $(\ket{v},\ket{v})\geq 0$ with equality if and only if $\ket{v} = 0$.\\
$
(A,A)=\operatorname{tr}(A^{\dagger}A)=\operatorname{tr}(\sum_{i}\lambda_{i}^{*}\lambda_{i}\ket{i}\bra{i})
=\operatorname{tr}(\sum_{i}||\lambda_{i}||^{2}\ket{i}\bra{i})
=\sum_{i}||\lambda_{i}||^{2}\operatorname{tr}(\ket{i}\bra{i})\\
=\sum_{i}||\lambda_{i}||^{2}\bra{i}\ket{i}
=\sum_{i}||\lambda_{i}||^{2} \geq 0$. When $A = 0,$ we can get $(A,A)=0$.
\\
Thus we proved that $(\ket{v},\ket{v})\geq 0$ with equality if and only if $\ket{v} = 0$.\\
Above all, the function (·, ·) on $L_{v}$× $L_{v}$ is an inner product function. \\
(2) Since $A=\sum_{i j} \bra{i}A\ket{j}\ket{i}\bra{j}=\sum_{i j} A_{ji}\ket{i}\bra{j}$,  then $\ket{i}\bra{j}$ as a set of bases. $ \ket{i}$ has $d$, then $\ket{i}\bra{j}$  has  $d^{2}$.
\\
(3)According to the second question, we can suppose that $\ket{i}\bra{j}$ is an orthonormal basis. Verification as follows:\\
 $(\ket{i}\bra{j}, \ket{i}\bra{j})=\operatorname{tr}(\ket{j}\bra{i}\ket{i}\bra{j})=\operatorname{tr}(\ket{j}\bra{j})
 = \bra{j}\ket{j}=1.
 $
\\
 $(\ket{i}\bra{j}, \ket{k}\bra{l})=\operatorname{tr}(\ket{j}\bra{i}\ket{k}\bra{l})=\operatorname{tr}(\ket{j}0\bra{j})
 =0, ( k, l \neq i, j).
 $\\
 Above all, thus $\ket{i}\bra{j}$ is the orthonormal basis.
\\ \\
\textbf{Exercise 2.40:  (Commutation relations for the Pauli matrices)} Verify the commutation relations
\begin{center}
$[X,Y]=2iZ; [Y,Z]=2iX; [Z,X]=2iY.$
\end{center}
\textbf{Answer:}	 \\
$[X,Y]=\begin{bmatrix}0& 1\\1& 0\end{bmatrix}\begin{bmatrix}0& -i\\i& 0\end{bmatrix}
-\begin{bmatrix}0& -i\\i& 0\end{bmatrix}\begin{bmatrix}0& 1\\1& 0\end{bmatrix}$\\
        $=\begin{bmatrix}i& 0\\0& -i\end{bmatrix}-\begin{bmatrix}-i& 0\\0& i\end{bmatrix}$\\
      $  =\begin{bmatrix}2i& 0\\0& -2i\end{bmatrix}=2iZ$.\\
$[Y,Z]=\begin{bmatrix}0& -i\\i& 0\end{bmatrix}\begin{bmatrix}1& 0\\0& -1\end{bmatrix}
-\begin{bmatrix}1& 0\\0& -1\end{bmatrix}\begin{bmatrix}0& -i\\i& 0\end{bmatrix}$\\
        $=\begin{bmatrix}0& i\\i& 0\end{bmatrix}-\begin{bmatrix}0& -i\\-i & 0\end{bmatrix}$\\
      $  =\begin{bmatrix}0& 2i\\2i& 0\end{bmatrix}=2iX$.\\
$[Z,Y]=\begin{bmatrix}1& 0\\0& -1\end{bmatrix}\begin{bmatrix}0& 1\\1& 0\end{bmatrix}
-\begin{bmatrix}0& -i\\i& 0\end{bmatrix}\begin{bmatrix}0& 1\\1& 0\end{bmatrix}$\\
        $=\begin{bmatrix}0& 1\\-1& 0\end{bmatrix}-\begin{bmatrix}0& -1\\1& 0\end{bmatrix}$\\
      $  =\begin{bmatrix}0& 2\\-2& 0\end{bmatrix}=2iY.$\\
Thus we verified the commutation relations for the Pauli matrices.






















\end{quote}

\end{document}